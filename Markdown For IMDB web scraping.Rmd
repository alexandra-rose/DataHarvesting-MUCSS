---
title: "Script"
author: "Alexandra Salo and Sof√≠a Villamil"
date: "2024-03-03"
output: html_document
---

# *Data Harvesting Script*

```{r}
rm(list = ls())
```

### Libraries

```{r}
library(rvest)
library(httr)
library(dplyr)
library(tidytext)
library(purrr)
library(ggplot2)
library(scales)
library(jsonlite)
library(tidyverse)
library(dplyr)
library(httr)
library(xml2)
library(magrittr)
library(scrapex)

```

### User Agent

```{r}
set_config( user_agent("
                       "))
  #set here your user agent
  useragent <-"
  "
#we will call on this variable later, also set your user agent here
```

## IMDB - Web scraping first stage

```{r}
link <- "https://www.imdb.com/chart/top"
```

```{r}
Top250_website <- link %>% read_html()
```

### Titles

```{r}
# Make an HTTP GET request with 'Accept-Language' header set to 'en'
response <- GET(link, add_headers("Accept-Language" = "en-US,en;q=0.5"))

# Parse the HTML content from the response
Top250_website <- content(response, "text") %>% read_html()

# Select the movie title nodes
movie_title_nodes <- html_nodes(Top250_website, "h3.ipc-title__text")

# Extracting movie titles
movie_titles <- html_text(movie_title_nodes)

length(movie_title_nodes) # I have node I don't need.

# Extracting the titles I need only
movie_titles <- html_text(movie_title_nodes)[2:251]
print(movie_titles)

```

## Reddit API scraping!

We will use the Reddit API to scrape comments about movie titles that we are interested in. In this case that is the 100 movie titles that we scraped from the IMBD website. We automate this using different functions and calling on all of them at the end in the main_function to collect a data frame that collects the name of movie that the comment is related to and the comments extracted from the discussion posts under the r/movies sub reddit. It also scrapes some sub comments.

A function to request a token from the Reddit API.

```{r}
#function to collect a token for the Reddit API
request_token <- function() {
  # Your Reddit app details
  client_id <- " " #personal script use, check readme file
  client_secret <- " " #secret code, check readme file
  username <- " " #your reddit email
  password <- " " #your Reddit password
 
  # Authenticate and get a token
  response <- POST(
    url = "https://www.reddit.com/api/v1/access_token",
    body = list(grant_type = "password", username = username, password = password),
    encode = "form",
    authenticate(client_id, client_secret),
    verbose()
  )

  # Extract token from response
  token <- content(response)$access_token
  token
}
```

This function is used to extract the unique ID from each permalink that is found in the JSON of each link.

```{r}
extract_id <- function(link) {
  match <- regmatches(link, regexpr("comments\\/(.{7})", link))
  if (length(match) > 0) {
    id <- substr(match[[1]], 10, 16)
    if (substr(id, nchar(id), nchar(id)) == "/") {
      id <- substr(id, 1, nchar(id) - 1)
    }
    return(id)
  } else {
    return(NA)  # Return NA if no match found
  }
}
```

A function to collect a list with the ID of the links of the posts.

```{r}
collect_post_links <- function(movietitles, token) {
  all_id_codes <- c()  # Initialize an empty vector to store all id codes
  
  for (title in movietitles) {
    search_url <- "https://api.reddit.com/r/movies/search"
    search_params <- list(
      q = title, 
      sort = "relevance",
      limit = 3, 
      restrict_sr = "ON"
    )
    
    req <- GET(
      url = search_url, 
      add_headers("Authorization" = paste("Bearer", token)), 
      add_headers("User-Agent" = useragent), 
      query = search_params
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(runif(1, 1, 3))
    
    parsed_content <- fromJSON(content(req, "text"))
    
    str(parsed_content)
    
    children_list <- parsed_content$data$children
    
    result <- children_list %>%
      as_tibble() %>%
      unnest(cols = data) %>%
      select(title, permalink)
    
    result <- result |> 
      rowwise() %>%
      mutate(id36 = extract_id(permalink)) |>
      select(id36) |> 
      unlist()  # Convert result to plain vector
    
    all_id_codes <- c(all_id_codes, result)  # Combine id codes from different movies
    all_id_codes <- unname(all_id_codes)
    
  }
  
  return(all_id_codes)
}
```

Two functions to collect the JSON and convert it to comments to extract.

```{r}
collect_JSONcomments <- function(IDcodes, token) {
  all_parsed_content <- list()  # Initialize an empty list to store parsed content
  
  for (IDcode in IDcodes) {
    # Define the URL for the endpoint
    endpoint <- "https://api.reddit.com/r/movies/comments/article"
    
    # Define query parameters
    query_params <- list(
      article = IDcode,
      limit = 3, 
      depth = 2
    )
    
    # Make the GET request
    req2 <- GET(
      url = endpoint,
      query = query_params,
      add_headers(
        "Authorization" = paste("Bearer", token),
        "User-Agent" = useragent
      )
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(runif(1, 1, 3))
    
    # Parse JSON content
    parsed_content2 <- fromJSON(content(req2, "text"))
    
    # Append parsed_content2 to the list
    all_parsed_content <- c(all_parsed_content, list(parsed_content2))
  }
  
  return(all_parsed_content)
}

traverse_comments <- function(parsed_content_list) {
  # Initialize an empty character vector to store comments
  all_comments <- character()
  
  # Helper function to recursively traverse comments
  traverse <- function(comment) {
    # Append the comment body to the vector
    all_comments <<- c(all_comments, comment$data$body)
    
    # Check if the comment has replies
    if (!is.null(comment$data$replies$data$children)) {
      # Iterate through the replies
      for (reply in comment$data$replies$data$children) {
        # Recursively call traverse_comments for each reply
        traverse(reply)
      }
    }
  }
  
  # Start traversing from top-level comments for each dataframe
  for (parsed_content in parsed_content_list) {
    for (comment in parsed_content$data$children) {
      traverse(comment)
    }
  }
  
  # Additional comment to add
  for (parsed_content in parsed_content_list) {
    additional_comment <- parsed_content$data$children[[1]]$data$selftext
    
    # Add the additional comment to the end of the vector
    all_comments <- c(additional_comment, all_comments)
  }
  
  # Return the character vector containing all comments
  return(all_comments)
}
```

Finally a function to bring it all together and loop through the movie titles and return one big data frame with movie title searched with and the comments and sub comments returned with each movie.

```{r}
main_function <- function(movietitles) {
   
   # Initialize variables
   all_combined_df <- list()
   
   # Get the token
   token <- request_token()
   
   # Loop through each movie title
   for (title in movietitles) {
     
     # Get the post link IDs for the current movie title
     link_IDs <- collect_post_links(c(title), token) 
     
     # Get the comment json for the current movie title
     parsed_content2 <- collect_JSONcomments(link_IDs, token)
     
     # Call the function to get all comments for the current movie title
     all_comments <- traverse_comments(parsed_content2)
     
     # Create a data frame for the current movie title with all its comments
     temp_df <- data.frame(movietitle = rep(title, length(all_comments)), all_comments =       all_comments, stringsAsFactors = FALSE)
     
     # Append the temporary data frame to the list
     all_combined_df[[title]] <- temp_df
   }
   
  # Combine all data frames in the list into one large data frame
   combined_df <- do.call(rbind, all_combined_df)
   
   # Reset the row names to be sequential numbers
   row.names(combined_df) <- NULL
   
   return(combined_df)
}

movietitle <- movie_titles[1:10]
data_reddit <- main_function(movietitle)
print(data_reddit)
```

## IMDB - Web scraping second stage

URL

```{r}
movie_link_nodes <- html_nodes(Top250_website, 'a.ipc-title-link-wrapper')
movie_urls <- html_attr(movie_link_nodes, 'href')

full_movie_urls <- paste0("https://www.imdb.com", movie_urls)
full_movie_urls <- full_movie_urls[1:250]

print(full_movie_urls)

```

### Getting the information for one specific URL

```{r}
url <- "https://www.imdb.com/title/tt23849204/?ref_=chttp_t_54" 

scrape_movie_details <- function(url) {
  movie_page <- read_html(url)
  #Title
  title <- movie_page %>%
    html_nodes('span.hero__primary-text') %>%
    html_text(trim = TRUE)

  # Year
  year <- movie_page %>%
    html_nodes('a.ipc-link--baseAlt[href*="/releaseinfo"]') %>%
    html_text(trim = TRUE) %>%
    .[1]

  # MPAA Rating
  mpaa_rating_node <- movie_page %>%
    html_nodes('li.ipc-inline-list__item a[href*="/parentalguide/certificates"]') 

  mpaa_rating <- if (length(mpaa_rating_node) > 0) html_text(mpaa_rating_node[1], trim = TRUE) else NA


  # Duration
  duration_nodes <- movie_page %>%
    html_nodes('ul.ipc-inline-list li.ipc-inline-list__item') %>%
    html_text(trim = TRUE)
  
  duration <- grep("^[0-9]+h [0-9]+m$", duration_nodes, value = TRUE)
  duration <- if (length(duration) > 0) tail(duration, n = 1) else NA

  # IMDb Score
  imdb_score <- movie_page %>%
    html_nodes('span.sc-bde20123-1') %>%
    html_text(trim = TRUE) %>%
    .[1]

  # Director
  director <- movie_page %>%
    html_nodes('a.ipc-metadata-list-item__list-content-item--link[href*="/name"]') %>%
    html_text(trim = TRUE) %>%
    .[1] 

  # First Actor
  first_actor <- movie_page %>%
    html_nodes('a[data-testid="title-cast-item__actor"]') %>%
    html_text(trim = TRUE) %>%
    .[1]

  # Second Actor
  second_actor <- movie_page %>%
    html_nodes('a[data-testid="title-cast-item__actor"]') %>%
    html_text(trim = TRUE) %>%
    .[2]

  # First Writer
  first_writer <- movie_page %>%
    html_nodes('a.ipc-metadata-list-item__list-content-item--link[href*="/name"]') %>%
    html_text(trim = TRUE) %>%
    .[2]

  # Second Writer
  second_writer <- movie_page %>%
    html_nodes('a.ipc-metadata-list-item__list-content-item--link[href*="/name"]') %>%
    html_text(trim = TRUE) %>%
    .[3] 
  
  # Release Date
  release_date <- movie_page %>%
    html_nodes('a.ipc-metadata-list-item__list-content-item--link[href*="/releaseinfo"]') %>%
    html_text(trim = TRUE)%>%
    .[1] 
  
  # Genres
  genre <- movie_page %>%
    html_nodes('a.ipc-chip.ipc-chip--on-baseAlt') %>%
    html_nodes('span.ipc-chip__text') %>%
    html_text(trim = TRUE)%>%
    .[1] 
 
  # Budget
  budget <- movie_page %>%
    html_node('[data-testid="title-boxoffice-budget"] .ipc-metadata-list-item__list-content-item') %>%
    html_text(trim = TRUE)

  # Gross Worldwide
  gross_worldwide <- movie_page %>%
    html_node('[data-testid="title-boxoffice-cumulativeworldwidegross"] .ipc-metadata-list-item__list-content-item') %>%
    html_text(trim = TRUE)

  
  # Title of the Feature Review
  review_title <- movie_page %>%
    html_node('span[data-testid="review-summary"]') %>%
    html_text(trim = TRUE)%>%
    .[1]
  
   # Feature Review
  feature_review <- movie_page %>%
    html_node('div[data-testid="review-overflow"] .ipc-html-content-inner-div') %>%
    html_text(trim = TRUE)

  # Number of Awards and Nominations
  awards_and_nominations <- movie_page %>%
    html_node('span.ipc-metadata-list-item__list-content-item') %>%
    html_text(trim = TRUE)
  
  oscars_nominations <- movie_page %>%
    html_node('a.ipc-metadata-list-item__label--link[aria-label="See more awards and nominations"]')%>%
    html_text(trim = TRUE)

  list(
    Title = title, Year = year, MPAA = mpaa_rating, Duration = duration, IMDbscore = imdb_score, Director = director,
    FirstActor = first_actor, SecondActor = second_actor, FirstWriter = first_writer,
    SecondWriter = second_writer, ReleaseDate = release_date, Genre=genre, Budget = budget,
    GrossWorldwide = gross_worldwide, TitleReview = review_title, FeatureReview = feature_review,
    AwardsAndNominations = awards_and_nominations, Oscar_Information = oscars_nominations
  )
}

movie_details <- scrape_movie_details(url)
print(movie_details)
```

### Testing for 5 URLS to see the data structure

```{r}
movies_df <- data.frame(
  Title = character(),
  Year = character(),
  MPAA = character(),
  Duration = character(),
  IMDbscore = character(),
  Director = character(),
  FirstActor = character(),
  SecondActor = character(),
  FirstWriter = character(),
  SecondWriter = character(),
  ReleaseDate = character(),
  Genre = character(),
  Budget = character(),
  GrossWorldwide = character(),
  TitleReview = character(),
  FeatureReview = character(),
  AwardsAndNominations = character(),
  Oscar_Information = character(),
  stringsAsFactors = FALSE
)

test_urls <- head(full_movie_urls, 5)

for (url in test_urls) {
  print(paste("Processing:", url))
  Sys.sleep(runif(1, 1, 3)) 
  movie_details <- tryCatch({
    scrape_movie_details(url)
  }, error = function(e) {
    print(paste("Error in processing:", url))
    print(e)
    return(NULL)
  })
  
  if (!is.null(movie_details)) {
    movie_details_df <- as.data.frame(matrix(unlist(movie_details), nrow = 1, byrow = TRUE), stringsAsFactors = FALSE)
    colnames(movie_details_df) <- names(movie_details)
    movies_df <- rbind(movies_df, movie_details_df)
  }
}

print(movies_df)


```

### Trying for every URL

```{r}
movies_df_final <- data.frame(
  Title = character(),
  Year = character(),
  MPAA = character(),
  Duration = character(),
  IMDbscore = character(),
  Director = character(),
  FirstActor = character(),
  SecondActor = character(),
  FirstWriter = character(),
  SecondWriter = character(),
  ReleaseDate = character(),
  Genre = character(),
  Budget = character(),
  GrossWorldwide = character(),
  TitleReview = character(),
  FeatureReview = character(),
  AwardsAndNominations = character(),
  Oscar_Information = character(),
  stringsAsFactors = FALSE
)

for (url in full_movie_urls) {
  print(paste("Processing:", url))
  Sys.sleep(runif(1, 1, 3)) 
  movie_details <- tryCatch({
    scrape_movie_details(url)
  }, error = function(e) {
    print(paste("Error in processing:", url))
    print(e)
    return(NULL)
  })
  
  if (!is.null(movie_details)) {
    movie_details_df <- as.data.frame(matrix(unlist(movie_details), nrow = 1, byrow = TRUE), stringsAsFactors = FALSE)
    colnames(movie_details_df) <- names(movie_details)
    movies_df_final <- rbind(movies_df_final, movie_details_df)
  }
}

print(movies_df_final)
```

## Sensitive analysis of the Feature Reviews

```{r}
# the objective is to perform a sentiment analysis for each row of reviews we attain for each movie. Thus creating a variable that classify the sentiment of each review if it is positive or negative, and in some instances neutral.

Review_Sentiment <- function(review) {
  sentiment_score <- data_frame(text = review) %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments("bing"), by = "word") %>%
    summarise(net_sentiment = sum(case_when(
      sentiment == "positive" ~ 1,
      sentiment == "negative" ~ -1,
      TRUE ~ 0
    ))) %>%
    pull(net_sentiment)
  
  if (sentiment_score > 0) {
    return("positive")
  } else if (sentiment_score < 0) {
    return("negative")
  } else {
    return("neutral") 
  }
}

movies_df_final <- movies_df_final %>%
  mutate(ReviewClassification = map_chr(FeatureReview, Review_Sentiment))

movies_df_final
```

## Sensitive analysis of the Reddit Comments

```{r}
data_reddit
```

Cleaning the data

```{r}
data_reddit$movietitle <- gsub("\\d+\\.\\s*", "", data_reddit$movietitle)
data_reddit$movietitle <- trimws(data_reddit$movietitle)
```

```{r}
data_reddit <- na.omit(data_reddit)
data_reddit <- data_reddit[nchar(trimws(data_reddit$all_comments)) > 0, ]
```

```{r}
Review_Sentiment_Reddit<- function(review) {
  sentiment_score_reddit <- data_frame(text = review) %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments("bing"), by = "word") %>%
    summarise(net_sentiment = sum(case_when(
      sentiment == "positive" ~ 1,
      sentiment == "negative" ~ -1,
      TRUE ~ 0
    ))) %>%
    pull(net_sentiment)
  
  if (sentiment_score_reddit > 0) {
    return("positive")
  } else if (sentiment_score_reddit < 0) {
    return("negative")
  } else {
    return("neutral") 
  }
}

data_reddit <- data_reddit %>%
  mutate(ReviewClassificationReddit = map_chr(all_comments, Review_Sentiment_Reddit))
```

## Descriptive analysis to understand the distribution of the data set.

```{r}
#Graph that shows for the Top three movies how many positve and negative comments they have in Reddit.

first_three_movies <- data_reddit %>% 
  filter(movietitle %in% unique(data_reddit$movietitle)[1:3])

sentiment_counts <- first_three_movies %>% 
  group_by(movietitle, ReviewClassificationReddit) %>% 
  summarise(count = n(), .groups = 'drop')


ggplot(sentiment_counts, aes(x = movietitle, y = count, fill = ReviewClassificationReddit)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  scale_fill_manual(values = c("negative" = "#8dd3c7", "neutral" = "#ffffb3", "positive" = "#bebada")) +
  labs(x = "Movie Title", y = "Number of Reviews", fill = "Sentiment") +
  ggtitle("Sentiment Count for the First Three Movies") +
  theme_minimal() +
  theme(text = element_text(family = "sans"), 
        plot.title = element_text(family = "sans"),
        axis.title = element_text(family = "sans"),
        axis.text = element_text(family = "sans")) 

  

```

```{r}
# Graph of the top genres of movies in my data set with their respective count of reviews sentiments

top_n_genres <- 6

top_genres <- movies_df_final %>%
  count(Genre, sort = TRUE) %>%
  top_n(top_n_genres, n)

filtered_movies <- movies_df_final %>%
  filter(Genre %in% top_genres$Genre)

Plot <- ggplot(filtered_movies, aes(x = ReviewClassification, fill = ReviewClassification)) +
  geom_bar(position = position_dodge(), color = "grey") +
  facet_wrap(~ Genre, scales = "free_x") +
  theme_light() +
  labs(title = "Top Genres by Review Classification",
       x = NULL, y = NULL) +
  scale_fill_manual(values = c("negative" = "#8dd3c7", "neutral" = "#ffffb3", "positive" = "#bebada")) +
theme(
    strip.text.x = element_text(size = 12, family = "sans"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, family = "sans"),
    axis.text.y = element_text(family = "sans"),
    panel.spacing = unit(1, "lines"),
    plot.title = element_text(hjust = 0.5, face = "bold", family = "sans"),
    legend.position = "none"
  )
print(Plot)

```

```{r}
# distribution of the movies per decade
year <- as.numeric(movies_df_final$Year)
movies_df_final <- movies_df_final %>%
  mutate(Decade = floor(year / 10) * 10) 

movies_per_decade <- movies_df_final %>%
  group_by(Decade) %>%
  summarise(Count = n()) %>%
  mutate(Decade = factor(Decade, levels = unique(Decade)))

Plot2 <- ggplot(movies_per_decade, aes(x = Decade, y = Count)) +
  geom_bar(stat = "identity", fill = "#8856a7", width = 0.7) +
  theme_light() +
  labs(title = "Number of Movies from the Top250 per Decade",
       x = NULL,
       y = NULL) +
  theme(
    plot.title = element_text(face = "bold", family = "sans", size = 14),
    axis.title = element_text(family = "sans", size = 11),
    axis.text = element_text(family = "sans", size = 10),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none"
  )

print(Plot2)


```
