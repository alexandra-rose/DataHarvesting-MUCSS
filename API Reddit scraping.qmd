---
title: "API Reddit scraping"
format: html
editor: visual
editor_options: 
  chunk_output_type: inline
---


## Reddit API scraping!

We will use the Reddit API to scrape comments about movie titles that we are interested in. In this case that is the 100 movie titles that we scraped from the IMBD website. We automate this using different functions and calling on all of them at the end in the main_function to collect a data frame that collects the name of movie that the comment is related to and the comments extracted from the discussion posts under the r/movies subreddit. It also scrapes some subcomments.


```{r}
library(jsonlite)
library(tidyverse)
library(dplyr)
library(httr)
library(xml2)
library(magrittr)
library(scrapex)

#set your own user agent
#set_config(
  #user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com"))

set_config(

  user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com")) #set your user agent

useragent <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com" #we will call on this variable later, also set your user agent here

  user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36; Sofia Villamil / sofia.v1999@gmail.com"))

  useragent <-"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36; Sofia Villamil / sofia.v1999@gmail.com"

# useragent <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com"

```

A function to request a token from the Reddit API.

```{r}
#function to collect a token for the Reddit API
request_token <- function() {
  # Your Reddit app details
  client_id <- "JDy0zGRa48MlaIQGNX1vPA"
  client_secret <- "mA178l-nYoxkbO3hJ1O297_NK5KJLQ"
  username <- "sofia.v1999@gmail.com"
  password <- "Solohc"
  #client_id <- "P6S7MQlutQirGAY3vxv7BQ" #personal script use, check readme file
  #client_secret <- "s1TY2MxqDrnsbRiGdSCokdSAOiSgFA" #secret code, check readme file
  #username <- "alexandra.rose.salo@gmail.com" #your reddit email
  #password <- "" #your Reddit password


  # Authenticate and get a token
  response <- POST(
    url = "https://www.reddit.com/api/v1/access_token",
    body = list(grant_type = "password", username = username, password = password),
    encode = "form",
    authenticate(client_id, client_secret),
    verbose()
  )

  # Extract token from response
  token <- content(response)$access_token
  token
}
```

This function is used to extract the unique ID from each permalink that is found in the JSON of each link.

```{r}
extract_id <- function(link) {
  match <- regmatches(link, regexpr("comments\\/(.{7})", link))
  if (length(match) > 0) {
    id <- substr(match[[1]], 10, 16)
    if (substr(id, nchar(id), nchar(id)) == "/") {
      id <- substr(id, 1, nchar(id) - 1)
    }
    return(id)
  } else {
    return(NA)  # Return NA if no match found
  }
}
```

A function to collect a list with the ID of the links of the posts.


```{r}
collect_post_links <- function(movietitles, token) {
  all_id_codes <- c()  # Initialize an empty vector to store all id codes
  
  for (title in movietitles) {
    search_url <- "https://api.reddit.com/r/movies/search"
    search_params <- list(
      q = title, 
      sort = "relevance",
      limit = 3, 
      restrict_sr = "ON"
    )
    
    req <- GET(
      url = search_url, 
      add_headers("Authorization" = paste("Bearer", token)), 
      add_headers("User-Agent" = useragent), 
      query = search_params
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(8)
    
    parsed_content <- fromJSON(content(req, "text"))
    
    str(parsed_content)
    
    children_list <- parsed_content$data$children
    
    result <- children_list %>%
      as_tibble() %>%
      unnest(cols = data) %>%
      select(title, permalink)
    
    result <- result |> 
      rowwise() %>%
      mutate(id36 = extract_id(permalink)) |>
      select(id36) |> 
      unlist()  # Convert result to plain vector
    
    all_id_codes <- c(all_id_codes, result)  # Combine id codes from different movies
    all_id_codes <- unname(all_id_codes)
    
  }
  
  return(all_id_codes)
}
```

Two functions to collect the JSON and convert it to comments to extract.


```{r}
collect_JSONcomments <- function(IDcodes, token) {
  all_parsed_content <- list()  # Initialize an empty list to store parsed content
  
  for (IDcode in IDcodes) {
    # Define the URL for the endpoint
    endpoint <- "https://api.reddit.com/r/movies/comments/article"
    
    # Define query parameters
    query_params <- list(
      article = IDcode,
      limit = 3, 
      depth = 2
    )
    
    # Make the GET request
    req2 <- GET(
      url = endpoint,
      query = query_params,
      add_headers(
        "Authorization" = paste("Bearer", token),
        "User-Agent" = useragent
      )
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(8)
    
    # Parse JSON content
    parsed_content2 <- fromJSON(content(req2, "text"))
    
    # Append parsed_content2 to the list
    all_parsed_content <- c(all_parsed_content, list(parsed_content2))
  }
  
  return(all_parsed_content)
}

traverse_comments <- function(parsed_content_list) {
  # Initialize an empty character vector to store comments
  all_comments <- character()
  
  # Helper function to recursively traverse comments
  traverse <- function(comment) {
    # Append the comment body to the vector
    all_comments <<- c(all_comments, comment$data$body)
    
    # Check if the comment has replies
    if (!is.null(comment$data$replies$data$children)) {
      # Iterate through the replies
      for (reply in comment$data$replies$data$children) {
        # Recursively call traverse_comments for each reply
        traverse(reply)
      }
    }
  }
  
  # Start traversing from top-level comments for each dataframe
  for (parsed_content in parsed_content_list) {
    for (comment in parsed_content$data$children) {
      traverse(comment)
    }
  }
  
  # Additional comment to add
  for (parsed_content in parsed_content_list) {
    additional_comment <- parsed_content$data$children[[1]]$data$selftext
    
    # Add the additional comment to the end of the vector
    all_comments <- c(additional_comment, all_comments)
  }
  
  # Return the character vector containing all comments
  return(all_comments)
}
```

Finally a function to bring it all together and loop through the movietitles and return one big data frame with movietitle searched with and the comments and subcomments returned with each movie.

```{r}
main_function <- function(movietitle) {
   
   #get the token
   token <- request_token()
   
   #get the post link IDs
   link_IDs <- collect_post_links(movietitle, token) 
   
   #get the comment json
   parsed_content2 <- collect_JSONcomments(link_IDs, token)
   
   # Call the function to get all comments
   all_comments <- traverse_comments(parsed_content2)
   
   #Create a data frame
   combined_df <- data.frame(movietitle = movietitle, all_comments = all_comments)
   
   # View the comments vector
   print(combined_df)
   
   return(combined_df)
}

#for top 100 movies
movietitle <- movie_titles[1:100]
trial2 <- main_function(movietitle)
print(trial2)


```
