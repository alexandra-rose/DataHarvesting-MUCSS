---
title: "API Reddit scraping"
format: html
editor: visual
editor_options: 
  chunk_output_type: inline
---

## 

```{r}
library(jsonlite)
library(tidyverse)
library(dplyr)
library(httr)
library(xml2)
library(magrittr)
library(scrapex)

#set your own user agent
set_config(
  user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com"))

useragent <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com"
```

The vibe

Main function: Collect comments (movie title name)

collect token

collect links for top 10 discussion posts (movie title name)

collect comments (link for

```{r}
#function to collect a token for the Reddit API
request_token <- function() {
  # Your Reddit app details
  client_id <- "P6S7MQlutQirGAY3vxv7BQ"
  client_secret <- "s1TY2MxqDrnsbRiGdSCokdSAOiSgFA"
  username <- "alexandra.rose.salo@gmail.com"
  password <- "AmericanMom26"

  # Authenticate and get a token
  response <- POST(
    url = "https://www.reddit.com/api/v1/access_token",
    body = list(grant_type = "password", username = username, password = password),
    encode = "form",
    authenticate(client_id, client_secret),
    verbose()
  )

  # Extract token from response
  token <- content(response)$access_token
  token
}


token <- request_token()

```

```{r}
extract_id <- function(link) {
  match <- regmatches(link, regexpr("comments\\/(.{7})", link))
  if (length(match) > 0) {
    id <- substr(match[[1]], 10, 16)
    if (substr(id, nchar(id), nchar(id)) == "/") {
      id <- substr(id, 1, nchar(id) - 1)
    }
    return(id)
  } else {
    return(NA)  # Return NA if no match found
  }
}

```

\
A function to collect a dataframe with the ID of the links of the posts and the movie name it was found with

```{r}
collect_post_links <- function(movietitles, token) {
  all_id_codes <- c()  # Initialize an empty vector to store all id codes
  
  for (title in movietitles) {
    search_url <- "https://api.reddit.com/r/movies/search"
    search_params <- list(
      q = title, 
      sort = "relevance",
      limit = 3, 
      restrict_sr = "ON"
    )
    
    req <- GET(
      url = search_url, 
      add_headers("Authorization" = paste("Bearer", token)), 
      add_headers("User-Agent" = useragent), 
      query = search_params
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(5)
    
    parsed_content <- fromJSON(content(req, "text"))
    
    str(parsed_content)
    
    children_list <- parsed_content$data$children
    
    result <- children_list %>%
      as_tibble() %>%
      unnest(cols = data) %>%
      select(title, permalink)
    
    result <- result |> 
      rowwise() %>%
      mutate(id36 = extract_id(permalink)) |>
      select(id36) |> 
      unlist()  # Convert result to plain vector
    
    all_id_codes <- c(all_id_codes, result)  # Combine id codes from different movies
    all_id_codes <- unname(all_id_codes)
    
  }
  
  return(all_id_codes)
}

#trial1 <- collect_post_links(movie_titles[1:10], token)
#trial1
```

\
Get the JSON output from the posts page

```{r, eval = FALSE}
# Define the URL for the endpoint
endpoint <- "https://api.reddit.com/r/movies/comments/article"
 
 # Define query parameters
query_params <- list(
   article = "tlwto3",
   limit = 5, 
   depth = 3
 )
 
 # Make the GET request
 req2 <- GET(
   url = endpoint,
   query = query_params,
   add_headers(
     "Authorization" = paste("Bearer", token),
     "User-Agent" = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36; Alexandra Salo / alexandra.rose.salo@gmail.com"
   )
 )
 
 # Parse JSON content
 parsed_content2 <- fromJSON(content(req2, "text"))
 
 # View the structure of the parsed content
 str(parsed_content2)
 
 # Define a function to traverse comments and return them as a character vector
traverse_comments <- function(comment) {
  # Initialize an empty character vector to store comments
  all_comments <- character()
  
  # Helper function to recursively traverse comments
  traverse <- function(comment) {
    # Append the comment body to the vector
    all_comments <<- c(all_comments, comment$data$body)
    
    # Check if the comment has replies
    if (!is.null(comment$data$replies$data$children)) {
      # Iterate through the replies
      for (reply in comment$data$replies$data$children) {
        # Recursively call traverse_comments for each reply
        traverse(reply)
      }
    }
  }
  
  # Start traversing from top-level comments
  for (comment in parsed_content2$data$children) {
    traverse(comment)
  }
  
  # Additional comment to add
  additional_comment <- parsed_content2$data$children[[1]]$data$selftext
  
  # Add the additional comment to the end of the vector
  all_comments <- c(additional_comment, all_comments)
  
  # Return the character vector containing all comments
  return(all_comments)
}

# Call the function to get all comments
all_comments <- traverse_comments(parsed_content2)

# View the comments vector
print(all_comments)
 
```

```{r}
collect_JSONcomments <- function(IDcodes, token) {
  all_parsed_content <- list()  # Initialize an empty list to store parsed content
  
  for (IDcode in IDcodes) {
    # Define the URL for the endpoint
    endpoint <- "https://api.reddit.com/r/movies/comments/article"
    
    # Define query parameters
    query_params <- list(
      article = IDcode,
      limit = 3, 
      depth = 3
    )
    
    # Make the GET request
    req2 <- GET(
      url = endpoint,
      query = query_params,
      add_headers(
        "Authorization" = paste("Bearer", token),
        "User-Agent" = useragent
      )
    )
    
    #take a break so Reddit doesnt kick me out
    Sys.sleep(5)
    
    # Parse JSON content
    parsed_content2 <- fromJSON(content(req2, "text"))
    
    # Append parsed_content2 to the list
    all_parsed_content <- c(all_parsed_content, list(parsed_content2))
  }
  
  return(all_parsed_content)
}

traverse_comments <- function(parsed_content_list) {
  # Initialize an empty character vector to store comments
  all_comments <- character()
  
  # Helper function to recursively traverse comments
  traverse <- function(comment) {
    # Append the comment body to the vector
    all_comments <<- c(all_comments, comment$data$body)
    
    # Check if the comment has replies
    if (!is.null(comment$data$replies$data$children)) {
      # Iterate through the replies
      for (reply in comment$data$replies$data$children) {
        # Recursively call traverse_comments for each reply
        traverse(reply)
      }
    }
  }
  
  # Start traversing from top-level comments for each dataframe
  for (parsed_content in parsed_content_list) {
    for (comment in parsed_content$data$children) {
      traverse(comment)
    }
  }
  
  # Additional comment to add
  for (parsed_content in parsed_content_list) {
    additional_comment <- parsed_content$data$children[[1]]$data$selftext
    
    # Add the additional comment to the end of the vector
    all_comments <- c(additional_comment, all_comments)
  }
  
  # Return the character vector containing all comments
  return(all_comments)
}


#trial <- collect_JSONcomments(trial1, token)
#trial

#traverse_comments(trial)
```

```{r, eval = FALSE}
# Define a function to traverse comments and return them as a character vector
traverse_comments <- function(comment) {
  # Initialize an empty character vector to store comments
  all_comments <- character()
  
  # Helper function to recursively traverse comments
  traverse <- function(comment) {
    # Append the comment body to the vector
    all_comments <<- c(all_comments, comment$data$body)
    
    # Check if the comment has replies
    if (!is.null(comment$data$replies$data$children)) {
      # Iterate through the replies
      for (reply in comment$data$replies$data$children) {
        # Recursively call traverse_comments for each reply
        traverse(reply)
      }
    }
  }
  
  # Start traversing from top-level comments
  for (comment in comment$data$children) {
    traverse(comment)
  }
  
  # Additional comment to add
  additional_comment <- comment$data$children[[1]]$data$selftext
  
  # Add the additional comment to the end of the vector
  all_comments <- c(additional_comment, all_comments)
  
  # Return the character vector containing all comments
  return(all_comments)
}

# Call the function to get all comments
all_comments <- traverse_comments(parsed_content2)

# View the comments vector
print(all_comments)



trial <- traverse_comments(parsed_content)
trial
```

```{r}
main_function <- function(movietitle) {
   
   #get the token
   token <- request_token()
   
   #get the post link IDs
   link_IDs <- collect_post_links(movietitle, token) 
   
   #get the comment json
   parsed_content2 <- collect_JSONcomments(link_IDs, token)
   
   # Call the function to get all comments
   all_comments <- traverse_comments(parsed_content2)
   
   #Create a data frame
   combined_df <- data.frame(movietitle = movietitle, all_comments = all_comments)
   
   # View the comments vector
   print(combined_df)
   
   return(combined_df)
}

movietitle <- movie_titles[1:3]
trial2 <- main_function(movietitle)


```
